{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO YOU USE GITHUB?**  \n",
    "If True: print('Remember to make your edits in a personal copy of this notebook')  \n",
    "Else: print('You don't have to understand. Continue your life.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Web Scraping 2\n",
    "\n",
    "In module_6 your learned some powerful tricks. Tricks that will work when the data is already shipped in a neat format. However this is often not the case. In this session we shall learn the art of parsing unstructured text and a more principled and advanced method of parsing HTML.\n",
    "\n",
    "This will help you build ***custom datasets*** within just a few hours or days work, that would have taken ***months*** to curate and clean manually.\n",
    "\n",
    "\n",
    "\n",
    "Readings for `module 6+7+8`:\n",
    "- [Python for Data Analysis, chapter 6](https://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf)\n",
    "- [A Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [An introduction to web scraping with Python](https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5)\n",
    "- [Introduction to Web Scraping using Selenium](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72)\n",
    "\n",
    "Video materiale from `ISDS 2020`:\n",
    "- [Web Scraping 1](https://bit.ly/ISDS2021_6)\n",
    "- [Web Scraping 2](https://bit.ly/ISDS2021_7)\n",
    "- [Web Scraping 3](https://bit.ly/ISDS2021_8)\n",
    "\n",
    "Other ressources:\n",
    "- [Nicklas Webpage](https://nicklasjohansen.netlify.app/)\n",
    "- [Data Driven Organizational Analysis, Fall 2021](https://efteruddannelse.kurser.ku.dk/course/2021-2022/ASTK18379U)\n",
    "- [Master of Science (MSc) in Social Data Science](https://www.socialdatascience.dk/education)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to HTML\n",
    "[What is HTML?](https://www.w3schools.com/whatis/whatis_html.asp)  \n",
    "\n",
    "HTML has a Tree structure. \n",
    "\n",
    "Each node in the tree has:\n",
    "- Children, siblings, parents, descendants. \n",
    "- Ids and attributes\n",
    "\n",
    "<img src=\"http://www.openbookproject.net/tutorials/getdown/css/images/lesson4/HTMLDOMTree.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important syntax and patterns\n",
    "_______________\n",
    "```html \n",
    "<p>The p tag indicates a paragraph <p/>\n",
    "```\n",
    "_______________\n",
    "```html \n",
    "<b>The b tag makes the text bold, giving us a clue to its importance </b>\n",
    "```\n",
    "output: <b>The b tag makes the text bold, giving us a clue to its importance </b>\n",
    "```html \n",
    "\n",
    "<em>The em tag emphasize the text</em>, giving us a clue to its importance\n",
    "```\n",
    "output: <em>The em tag makes emphasize the text</em>, giving us a clue to its importance\n",
    "___________\n",
    "```html \n",
    "<h1>h1</h1><h2>h2</h2><h3>h3</h3><b>Headers give similar clues</b>\n",
    "```\n",
    "output:\n",
    "<h1>h1</h1><h2>h2</h2><h3>h3</h3><b>Headers give similar clues</b>  \n",
    "  \n",
    "```html \n",
    "<a href=\"www.google.com\">The a tag creates a hyperlink <a/>\n",
    "```\n",
    "output: <a href=\"www.google.com\">The a tag creates a hyperlink <a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we find our way around this tree?\n",
    "1. Regex: Extracting string patterns using .split and regular expresssions\n",
    "2. CSS-selectors: Specifying paths using css-selectors, xpath syntax.\n",
    "3. ```BeautifulSoup```: A more powerful, principled and readable way to parse data and navigate HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex\n",
    "- [What is regex?](https://en.wikipedia.org/wiki/Regular_expression)\n",
    "- The brute force way is to parse by convering your downloded matriale into a large string\n",
    "- Now you can create standard string operations\n",
    "- And apply smart regex to identify the data you are looking for e.g. links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/us-news/2019/aug/14/taco-eating-contest-death-fresno-california'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "#html.split('\\n')\n",
    "#re.findall(\"(?P<url>https?://[^\\s]+)\", html)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selectors \n",
    "- [What is a CSS Selector?](https://www.w3schools.com/css/css_selectors.asp)\n",
    "- Another way to browse through the HTML tree\n",
    "- Define a unique path to an element in the HTML tree.\n",
    "- It is quick but has to be hardcoded and also more likely to break.\n",
    "- [Nicklas recommend using this free Google Chrome CSS Selector](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Man dies after taco-eating contest in California'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.theguardian.com/us-news/2019/aug/14/taco-eating-contest-death-fresno-california'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.select('.dcr-125vfar')[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "BeautifulSoup makes the html tree navigable. \n",
    "It allows you to:\n",
    "    * Search for elements by tag name and/or by attribute.\n",
    "    * Iterate through them, go up, sideways or down the tree.\n",
    "    * Furthermore it helps you with standard tasks such as extracting raw text from html,\n",
    "    which would be a very tedious task if you had to hardcode it using `.split` commands and using your own regular expressions will be unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/us-news/2019/aug/14/taco-eating-contest-death-fresno-california'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#maincontent',\n",
       " '#navigation',\n",
       " '/preference/edition/int',\n",
       " '/preference/edition/uk',\n",
       " '/preference/edition/us']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: finding hyperlinks\n",
    "links = soup.find_all('a') # find all a tags -connoting a hyperlink.\n",
    "[link['href'] for link in links if link.has_attr('href')][0:5] # unpack the hyperlink from the a nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcr-125vfar : Man dies after taco-eating contest in California\n"
     ]
    }
   ],
   "source": [
    "# example: finding headline\n",
    "headline = soup.find('h1') # search for the first headline: h1 tag. \n",
    "name = headline['class'][0].strip() # use the class attribute name as column name.\n",
    "value = headline.text.strip() # extract text using build in method.\n",
    "print(name,':',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man died shortly after competing in a taco-eating contest at a minor league baseball game in California, authorities said Wednesday.Dana Hutchings, 41, of Fresno, died Tuesday night shortly after arriving at a hospital, said Tony Botti, a Fresno sheriff spokesman.An autopsy on Hutchings will be done Thursday to determine a cause of death, Botti said. It was not immediately known how many tacos the man had eaten or whether he had won the contest.Paul Braverman, a spokesman for the Fresno Grizzlies, said in a statement that the team was “devastated to learn” of the fan’s death and that the team would “work closely with local authorities and provide any helpful information that is requested”.Tuesday night’s competition came before Saturday’s World Taco Eating Championship, to be held at Fresno’s annual Taco Truck Throwdown. The team on Wednesday announced that it was canceling that contest.Matthew Boylan, who watched Tuesday’s taco eating contest from his seat in the stadium, told the Fresno Bee he quickly noticed Hutchings because “he was eating so fast compared to the other two [contestants]”.“It was like he’d never eaten before,” Boylan said. “He was just shoving the tacos down his mouth without chewing.”He said Hutchings collapsed and hit his face on a table about seven minutes into the contest, then fell to the ground. The eating contest ended immediately.During the 2018 Taco Eating Championship in Fresno, the professional eater Geoffrey Esper downed 73 tacos in eight minutes, KFSN-TV reported.Competitive-eating contests have become major attractions at festivals and other events. Among the most popular is the annual Nathan’s Famous July Fourth hot dog eating contest on New York’s Coney Island, where Joey Chestnut, this year’s champion, ate 71. Esper finished third. TopicsCalifornianewsReuse this content'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: finding article_text\n",
    "article_text = soup.findAll('div', {'class':'dcr-185kcx9'})[0]\n",
    "article_text.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we are interested in how articles cite sources to back up their story i.e. their hyperlink behaviour within the article, and we want to see if the media has changed their behaviour.\n",
    "\n",
    "We know how to search for links. But the cool part is that we can search from anywhere in the HTML tree. This means that once we have located the article content node - as above - we can search from there. This results in hyperlinks used within the article text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.theguardian.com/us-news/california\n",
      "https://www.theguardian.com/us-news/gallery/2016/jul/04/nathans-famous-hotdog-eating-contest-in-pictures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.theguardian.com/us-news/california',\n",
       " 'https://www.theguardian.com/us-news/gallery/2016/jul/04/nathans-famous-hotdog-eating-contest-in-pictures']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: finding citation links\n",
    "citations = article_text.findAll('a')\n",
    "\n",
    "citation_links = [] # define container to the hyperlinks\n",
    "for citation in citations: # iterate through each citation node\n",
    "    if citation.has_attr('data-link-name'): # check if it has the right attribute\n",
    "        if citation['data-link-name'] =='in body link': # and if the value of that attribute is correct\n",
    "            print(citation['href'])\n",
    "            citation_links.append(citation['href']) #  add link to the container\n",
    "\n",
    "citation_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset from www.bold.dk\n",
    "\n",
    "### Let's put together some of the stuff we have learned so far\n",
    "1. **Investigate:** In this example we will try to investigate the website to uderstand its structure. \n",
    "2. **Mapping:** Then we will try to collect all the urls and save them into a list\n",
    "3. **Parsing:** At last, we will try to collect the information in each url in a simpel loop.\n",
    "\n",
    "#### First, we pay around with the site trying to understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSV-anfører vil ikke kalde FCM en walkover\n",
      "\n",
      "22:30\n",
      "https://www.bold.dk/fodbold/nyheder/psv-anfoerer-vil-ikke-kalde-fcm-en-walkover/\n"
     ]
    }
   ],
   "source": [
    "# define our URL\n",
    "url = 'https://www.bold.dk/' \n",
    "\n",
    "# connects to site\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse data with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text,'lxml') # parse the HTML\n",
    "\n",
    "#identify articles to scrape by inspecting site\n",
    "articles = soup.find_all('div',{'class':'news_list_item'}) # search for the ul node\n",
    "\n",
    "# checking if articles match website\n",
    "for i in range(1):\n",
    "    print(articles[i].text.strip())\n",
    "\n",
    "# identifying how to find an url from an article\n",
    "article_url = articles[0].attrs['data-vr-contentbox-url']\n",
    "print(article_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div article_id=\"389113\" class=\"news_list_item\" data-vr-contentbox=\"#1\" data-vr-contentbox-url=\"https://www.bold.dk/fodbold/nyheder/psv-anfoerer-vil-ikke-kalde-fcm-en-walkover/\" id=\"news_list_item_389113\" position=\"1\" tag_ids=\"246753,7968,8280\">\n",
       "<div class=\"checkbox ball no_text_select\" style=\"margin:7px 6px 3px 2px\">\n",
       "<img class=\"unchecked\" src=\"https://s3.eu-central-1.amazonaws.com/static.bold.dk/img/sprites/newslist_checkbox_20x40.png\"/>\n",
       "</div>\n",
       "<a class=\"title\" href=\"/fodbold/nyheder/psv-anfoerer-vil-ikke-kalde-fcm-en-walkover/\"><span data-vr-headline=\"\">PSV-anfører vil ikke kalde FCM en walkover</span>\n",
       "<img src=\"https://s3.eu-central-1.amazonaws.com/static.bold.dk/img/tag/180x180/8206.png\" style=\"position: absolute;width: 12px;right: 28px;top: 7px;\"/>\n",
       "<span class=\"font9 note-grey\" style=\"right:2px;\">22:30</span>\n",
       "</a>\n",
       "</div>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, we create a list of urls that we want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bold.dk/fodbold/nyheder/psv-anfoerer-vil-ikke-kalde-fcm-en-walkover/\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.bold.dk/' \n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "articles = soup.find_all('div',{'class':'news_list_item'})\n",
    "\n",
    "#create an empty list\n",
    "list_of_article_urls = []\n",
    "\n",
    "# creating a loop that appends the article url to the list above\n",
    "for i in range(len(articles)):\n",
    "    list_of_article_urls.append(articles[i].attrs['data-vr-contentbox-url'])\n",
    "\n",
    "#printing the list\n",
    "#list_of_article_urls\n",
    "\n",
    "#printing one example\n",
    "print(list_of_article_urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third, we scrape each site from the url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step usually reuqiere a new step of investigation\n",
    "# to figure out what information you want to download\n",
    "# in this example we want the title, the lead and time posted\n",
    "\n",
    "# creatig empty list for the infomation we want to extract for every article\n",
    "h1_list = []\n",
    "lead = []\n",
    "time_posted = []\n",
    "\n",
    "for i in range(10): # 10 #len(list_of_article_urls)\n",
    "    \n",
    "    # this time we scrape for each news article in the url list we created before\n",
    "    url = list_of_article_urls[i]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    \n",
    "    # pedagogical way of append title to list\n",
    "    temp_1 = soup.find_all('h1')\n",
    "    temp_1 = temp_1[1]\n",
    "    temp_1 = temp_1.text.strip()\n",
    "    h1_list.append(temp_1)\n",
    "    \n",
    "    # how I would actually do it\n",
    "    lead.append(soup.find_all('div',{'class':'lead'})[0].text.strip())\n",
    "    \n",
    "    # sometimes you make wierd things - that works\n",
    "    temp_3 = soup.find_all('time')\n",
    "    temp_3 = temp_3[0]\n",
    "    temp_3 = str(temp_3)[16:32]\n",
    "    time_posted.append(temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"break_new_headline\"></h1>,\n",
       " <h1>PL-klubber fortsætter knælen i ny sæson</h1>,\n",
       " <h1 class=\"title\">Fodbold - Seneste nyheder</h1>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h1 \n",
    "soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>PL-klubber fortsætter knælen i ny sæson</h1>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PL-klubber fortsætter knælen i ny sæson'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'De 20 Premier League-klubber er blevet enige om at knæle før kickoff i næste sæson også med budskabet om at få racisme ud af fodbolden.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lead\n",
    "soup.find_all('div',{'class':'lead'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<time datetime=\"2021-08-03 20:28\">03.08.2021 20:28</time>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time_posted\n",
    "soup.find_all('time')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, we put our collected information into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSV-anfører vil ikke kalde FCM en walkover</td>\n",
       "      <td>PSV-anfører Marco van Ginkel vil ikke kalde ka...</td>\n",
       "      <td>2021-08-03 22:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gerrard om overtidsmål: Et kæmpe øjeblik</td>\n",
       "      <td>Rangers-manager Steven Gerrard nød at se holde...</td>\n",
       "      <td>2021-08-03 22:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onuachu scorede forgæves i Shakhtar-triumf</td>\n",
       "      <td>Paul Onuachu bragte Genk foran, men det var ik...</td>\n",
       "      <td>2021-08-03 21:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSV blæste decimerede FCM omkuld</td>\n",
       "      <td>PSV satte et afbudsramt FC Midtjylland-hold på...</td>\n",
       "      <td>2021-08-03 21:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SLUT: PSV - FCM minut for minut</td>\n",
       "      <td>FC Midtjylland er uden flere profiler, når hol...</td>\n",
       "      <td>2021-08-03 21:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ajax belønner 18-årig dansker med ny aftale</td>\n",
       "      <td>Ajax har forlænget kontrakten med deres unge d...</td>\n",
       "      <td>2021-08-03 21:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overblik: Disse hold er videre i pokalen</td>\n",
       "      <td>Her får du overblikket over alle tirsdagens re...</td>\n",
       "      <td>2021-08-03 21:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fredericia er videre efter vildt pokal-drama</td>\n",
       "      <td>FC Fredericia er videre til anden runde i Sydb...</td>\n",
       "      <td>2021-08-03 20:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malmö-triumf: Rieks og AC dukkede Rangers</td>\n",
       "      <td>Søren Rieks scorede, og Anders Christiansen as...</td>\n",
       "      <td>2021-08-03 20:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PL-klubber fortsætter knælen i ny sæson</td>\n",
       "      <td>De 20 Premier League-klubber er blevet enige o...</td>\n",
       "      <td>2021-08-03 20:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0    PSV-anfører vil ikke kalde FCM en walkover   \n",
       "1      Gerrard om overtidsmål: Et kæmpe øjeblik   \n",
       "2    Onuachu scorede forgæves i Shakhtar-triumf   \n",
       "3              PSV blæste decimerede FCM omkuld   \n",
       "4               SLUT: PSV - FCM minut for minut   \n",
       "5   Ajax belønner 18-årig dansker med ny aftale   \n",
       "6      Overblik: Disse hold er videre i pokalen   \n",
       "7  Fredericia er videre efter vildt pokal-drama   \n",
       "8     Malmö-triumf: Rieks og AC dukkede Rangers   \n",
       "9       PL-klubber fortsætter knælen i ny sæson   \n",
       "\n",
       "                                                lead              time  \n",
       "0  PSV-anfører Marco van Ginkel vil ikke kalde ka...  2021-08-03 22:30  \n",
       "1  Rangers-manager Steven Gerrard nød at se holde...  2021-08-03 22:06  \n",
       "2  Paul Onuachu bragte Genk foran, men det var ik...  2021-08-03 21:55  \n",
       "3  PSV satte et afbudsramt FC Midtjylland-hold på...  2021-08-03 21:52  \n",
       "4  FC Midtjylland er uden flere profiler, når hol...  2021-08-03 21:47  \n",
       "5  Ajax har forlænget kontrakten med deres unge d...  2021-08-03 21:22  \n",
       "6  Her får du overblikket over alle tirsdagens re...  2021-08-03 21:10  \n",
       "7  FC Fredericia er videre til anden runde i Sydb...  2021-08-03 20:59  \n",
       "8  Søren Rieks scorede, og Anders Christiansen as...  2021-08-03 20:53  \n",
       "9  De 20 Premier League-klubber er blevet enige o...  2021-08-03 20:28  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'title':h1_list, 'lead':lead, 'time':time_posted})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSV-anfører vil ikke kalde FCM en walkover</td>\n",
       "      <td>PSV-anfører Marco van Ginkel vil ikke kalde ka...</td>\n",
       "      <td>2021-08-03 22:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gerrard om overtidsmål: Et kæmpe øjeblik</td>\n",
       "      <td>Rangers-manager Steven Gerrard nød at se holde...</td>\n",
       "      <td>2021-08-03 22:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onuachu scorede forgæves i Shakhtar-triumf</td>\n",
       "      <td>Paul Onuachu bragte Genk foran, men det var ik...</td>\n",
       "      <td>2021-08-03 21:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSV blæste decimerede FCM omkuld</td>\n",
       "      <td>PSV satte et afbudsramt FC Midtjylland-hold på...</td>\n",
       "      <td>2021-08-03 21:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SLUT: PSV - FCM minut for minut</td>\n",
       "      <td>FC Midtjylland er uden flere profiler, når hol...</td>\n",
       "      <td>2021-08-03 21:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ajax belønner 18-årig dansker med ny aftale</td>\n",
       "      <td>Ajax har forlænget kontrakten med deres unge d...</td>\n",
       "      <td>2021-08-03 21:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overblik: Disse hold er videre i pokalen</td>\n",
       "      <td>Her får du overblikket over alle tirsdagens re...</td>\n",
       "      <td>2021-08-03 21:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fredericia er videre efter vildt pokal-drama</td>\n",
       "      <td>FC Fredericia er videre til anden runde i Sydb...</td>\n",
       "      <td>2021-08-03 20:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malmö-triumf: Rieks og AC dukkede Rangers</td>\n",
       "      <td>Søren Rieks scorede, og Anders Christiansen as...</td>\n",
       "      <td>2021-08-03 20:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PL-klubber fortsætter knælen i ny sæson</td>\n",
       "      <td>De 20 Premier League-klubber er blevet enige o...</td>\n",
       "      <td>2021-08-03 20:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0    PSV-anfører vil ikke kalde FCM en walkover   \n",
       "1      Gerrard om overtidsmål: Et kæmpe øjeblik   \n",
       "2    Onuachu scorede forgæves i Shakhtar-triumf   \n",
       "3              PSV blæste decimerede FCM omkuld   \n",
       "4               SLUT: PSV - FCM minut for minut   \n",
       "5   Ajax belønner 18-årig dansker med ny aftale   \n",
       "6      Overblik: Disse hold er videre i pokalen   \n",
       "7  Fredericia er videre efter vildt pokal-drama   \n",
       "8     Malmö-triumf: Rieks og AC dukkede Rangers   \n",
       "9       PL-klubber fortsætter knælen i ny sæson   \n",
       "\n",
       "                                                lead              time  \n",
       "0  PSV-anfører Marco van Ginkel vil ikke kalde ka...  2021-08-03 22:30  \n",
       "1  Rangers-manager Steven Gerrard nød at se holde...  2021-08-03 22:06  \n",
       "2  Paul Onuachu bragte Genk foran, men det var ik...  2021-08-03 21:55  \n",
       "3  PSV satte et afbudsramt FC Midtjylland-hold på...  2021-08-03 21:52  \n",
       "4  FC Midtjylland er uden flere profiler, når hol...  2021-08-03 21:47  \n",
       "5  Ajax har forlænget kontrakten med deres unge d...  2021-08-03 21:22  \n",
       "6  Her får du overblikket over alle tirsdagens re...  2021-08-03 21:10  \n",
       "7  FC Fredericia er videre til anden runde i Sydb...  2021-08-03 20:59  \n",
       "8  Søren Rieks scorede, og Anders Christiansen as...  2021-08-03 20:53  \n",
       "9  De 20 Premier League-klubber er blevet enige o...  2021-08-03 20:28  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving df\n",
    "df.to_csv('df_bold.dk.csv')\n",
    "\n",
    "# loading df\n",
    "pd.read_csv('df_bold.dk.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 7: Web Scraping 2\n",
    "\n",
    "In this Exercise Set we shall develop our webscraping skills even further by practicing **parsing** and navigating html trees using `BeautifoulSoup` and furthermore train extracting information from raw text with no html tags to help, using regular expressions. \n",
    "\n",
    "But just as importantly you will get a chance to think about **data quality issues** and how to ensure reliability when curating your own webdata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.1: Logging and data quality\n",
    "\n",
    "> **Ex. 7.1.1:** *`Why` is it important to log processes in your data collection?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.2:**\n",
    "*`How` does logging help with both ensuring and documenting the quality of your data?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.2: Parsing a Table from HTML using BeautifulSoup.\n",
    "\n",
    "In module_6 I showed you a neat little prepackaged function in pandas that did all the work. However today we should learn the mechanics of it. *(It is not just for educational purposes, sometimes the package will not do exactly as you want.)*\n",
    "\n",
    "We hit the Basketball stats page from yesterday again: https://www.basketball-reference.com/leagues/NBA_2018.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.1:** Here we practice simply locating the table node of interest using the `find` method build into BeautifoulSoup. But first we have to fetch the HTML using the `requests` module. Parse the tree using `BeautifulSoup`. And then use the **>Inspector<** tool (* right click on the table < press inspect element *) in your browser to see how to locate the Eastern Conference table node - i.e. the *tag* name of the node, and maybe some defining *attributes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have located the table should now build a function that starts at a \"table node\" and parses the information, and outputs a pandas DataFrame. \n",
    "\n",
    "Inspect the element either within the notebook or through the **>Inspector<** tool and start to see how a table is written in html. Which tag names can be used to locate rows? How will you iterate through columns. Were is the header located?\n",
    "\n",
    "> **Ex. 7.2.2:** First you parse the header which can be found in the canonical tag name: thead. \n",
    "Next you use the `find_all` method to search for the tag, and iterate through each of the elements extracting the text, using the `.text` method builtin to the the node object. Store the header values in a list container. \n",
    "\n",
    "> **Ex. 7.2.3:** Next you locate the rows, using the canonical tag name: tbody. And from here you search for all rows tags. Fiugre out the tag name yourself, inspecting the tbody node in python or using the **Inspector**. \n",
    "\n",
    "> **Ex. 7.2.4:** Next run through all the rows and extract each value, similar to how you extracted the header. However here is a slight variation: Since each value node can have a different tag depending on whether it is a digit or a string, you should use the `.children` method instead of the `.find_all` - (or write compile a regex that matches both the td tag and the th tag.) \n",
    ">Once the value nodes of each row has been located using the `.children` method you should extract the value. Store the extracted rows as a list of lists: ```[[val1,val2,...valk],...]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.5:** Now locate all tables from the page, using the `.find_all` method searching for the table tag name. Iterate through the table nodes and apply the function created for parsing html tables. Store each table in a dictionary using the table name as key. The name is found by accessing the id attribute of each table node, using dictionary-style syntax - i.e. `table_node['id']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.6. (extra) :** Compare your results to the pandas implementation [pd.read_html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.3: The European Research Counsel (optional)\n",
    "**NOTE** Exercise 7.3 is difficult and therefore also optional. I expect less than 10% of you being able to solve this one.\n",
    "\n",
    "Imagine we wanted to analyze whether the European funding behaviour was biased towards certain countries and gender. We might decide to scrape who has received funding from the ERC.\n",
    "https://erc.europa.eu/\n",
    "\n",
    "* First we figure find navigate the grant listings.\n",
    "* Next we figure out how to page these results. \n",
    "* And finally we want to grab the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage and operating system interactions\n",
    "\n",
    "> **Ex. 7.3.1:** *Import the python library `os`. Write pyhon code in this jupyter notebook creating a new folder in your directory called \"erc_funding\". Inside your new folder create 3 subfolders called 'mapping', 'raw_data' and 'parsed_data'.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "> **Ex. 7.3.2:** *Investigate [https://erc.europa.eu/projects-figures/erc-funded-projects/results?items_per_page=100&search_api_views_fulltext=&](https://erc.europa.eu/projects-figures/erc-funded-projects/results?items_per_page=100&search_api_views_fulltext=&). Figure out how many sites you need to loop thorugh. Save the response for each site using in `condecs` in your \"mapping\" subfolder. Use the `tqdm` to track your loop.\n",
    "Use the Snorre Ralund Connector class to log your activity.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "> **Ex. 7.3.3:** *Write a function that takes a filename (from our mapping subfolder) as and input and returns (in our parsed_data subfolder) a `pandas`dataframe of parsed information. Use `os` library to navigate your operating system (paths) and `condecs` library to read files inside your function. Last, concatenate all your dataframes into one dataframe you call \"df\" consisting of all parsed data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability and Data Quality\n",
    "\n",
    "##### Inspect the data\n",
    "> **Ex. 7.3.4:** *Investigate your dataframe \"df\". Check for dublicates. Count NaN values. Create a `matplotlib` histrogram plot for every column of \"df\" illustrating the lenght of the string (x-axis) and row count( y-axis).*\n",
    "\n",
    "##### Do simple descriptives\n",
    "> **Ex. 7.3.5:** *Create a value_counts() for each of the three columns (Host Institution (HI), Researcher (PI) and Project acronym) in your \"df\". What can counting do for us in this exercise in terms of Reliability and Data Quality?*\n",
    "\n",
    "##### Visualize the Log\n",
    "> **Ex. 7.3.6:** *Load your \"erc_log.csv\". Convert the time column 't' to datetime. Use `matplotlib` to create three plots: (1) time it took to make the call, (2) the response size over time, and (3) the delta_t against the response_size .)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "328px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
